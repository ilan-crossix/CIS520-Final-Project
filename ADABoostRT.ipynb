{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from sklearn.decomposition import PCA\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import cross_validate,KFold\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "import csv\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import sklearn.model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilangold/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/ilangold/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/Users/ilangold/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "engine = sqlite3.connect('CleanData-Apr15')\n",
    "\n",
    "totaldata_df = pd.read_sql('select * from data', engine)\n",
    "train_data, test_data= ms.train_test_split(totaldata_df, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data[['CENTROID_X','CENTROID_Y','DistCBD','pop_density','housing_ratio']])\n",
    "train_data[['CENTROID_X','CENTROID_Y','DistCBD','pop_density','housing_ratio']] =scaler.transform(train_data[['CENTROID_X','CENTROID_Y','DistCBD','pop_density','housing_ratio']])\n",
    "test_data[['CENTROID_X','CENTROID_Y','DistCBD','pop_density','housing_ratio']] =scaler.transform(test_data[['CENTROID_X','CENTROID_Y','DistCBD','pop_density','housing_ratio']])\n",
    "\n",
    "X_train=train_data.drop(labels=['WIDTH'],axis='columns')\n",
    "y_train=train_data[['WIDTH']]\n",
    "X_test=test_data.drop(labels=['WIDTH'],axis='columns')\n",
    "y_test=test_data[['WIDTH']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adapted from R code found here: https://github.com/ron1818/PhD_code/blob/master/AdaBoost/myAdaBoostRT.R\n",
    "\n",
    "def AdaBoostRT(x, y, model, phi, xtest = None, maxIter=100):\n",
    "    (m, n) = np.shape(x)\n",
    "\n",
    "    T=maxIter # max iter\n",
    "    #initialize weight distr and error\n",
    "    epsilon = np.zeros(T)\n",
    "    beta = np.zeros(T)\n",
    "    D = np.zeros(T)\n",
    "    fit = np.zeros((T, m))\n",
    "    e = np.zeros((T, m))\n",
    "    w = np.zeros((T + 1, m))\n",
    "    w[0,:]= np.repeat(1/m, m)\n",
    "    models = [0 for t in range(T)]\n",
    "    t=0\n",
    "    while (t<(T)):\n",
    "        idx = np.random.choice(list(range(1, m+1)),  size = (1, m), replace = True, p = w[t, :])\n",
    "        tth_x=x.iloc[idx[0] - 1,:]\n",
    "        tth_y=y.iloc[idx[0] - 1,:]\n",
    "        # call weak learner, CART\n",
    "        models[t]=model.fit(tth_x,tth_y)\n",
    "        # getback to hypothesis\n",
    "        fit[t,:]=model.predict(x)\n",
    "        # adjusted error for each instance\n",
    "        y_np = np.transpose(np.asarray(y))\n",
    "        ARE=np.abs(np.divide((y_np - np.transpose(fit[t,:])),y_np))\n",
    "        is_ARE_gt_phi = ARE > phi\n",
    "        is_ARE_le_phi = ARE <= phi\n",
    "        # calculate error of hypothesis\n",
    "        epsilon[t] = np.sum(w[t,is_ARE_gt_phi[0]])\n",
    "        # calculate beta\n",
    "        beta[t]=epsilon[t] * epsilon[t]\n",
    "        # update weight vector\n",
    "        w[t+1,is_ARE_le_phi[0]]=w[t,is_ARE_le_phi[0]]*beta[t]\n",
    "        w[t+1,is_ARE_gt_phi[0]]=w[t,is_ARE_gt_phi[0]]\n",
    "        # normalize\n",
    "        w[t+1,]=np.divide(w[t+1,:],np.sum(w[t+1,:]))\n",
    "        t=t+1\n",
    "    # final model\n",
    "    max_t=t-1\n",
    "    ww=np.log(np.divide(1,beta[list(range(0, max_t))]))\n",
    "    aggregatedfit = np.divide((np.matmul(ww , fit[list(range(0, max_t)), :])) , np.sum(ww))\n",
    "    # predict\n",
    "    if xtest is None: # have test dataset\n",
    "        aggregatedpredict= None\n",
    "    else:\n",
    "        (mtest, n) = np.shape(xtest)\n",
    "        prediction = np.zeros((max_t,mtest))\n",
    "        for t in range(max_t):\n",
    "            prediction[t,:] = model.predict(xtest)\n",
    "        aggregatedpredict = np.matmul(ww, prediction) / sum(ww)\n",
    "    return aggregatedfit, aggregatedpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regr = DecisionTreeRegressor(random_state = 314, min_samples_split=68, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.1100182   2.29974971  6.72125316 ...,  2.29974971  3.1100182\n",
      "  6.72125316]\n",
      "PHI\n",
      "0.01\n",
      "err sq and err perc\n",
      "100.044416145\n",
      "11.2722551643\n",
      "[ 25.99384634  67.3127274    3.04506384 ...,   3.04506384  67.3127274\n",
      "  21.8772206 ]\n",
      "PHI\n",
      "0.05\n",
      "err sq and err perc\n",
      "183.709705388\n",
      "16.8181901477\n",
      "[ 3.18766793  3.18766793  3.18766793 ...,  3.18766793  3.18766793\n",
      "  3.18766793]\n",
      "PHI\n",
      "0.1\n",
      "err sq and err perc\n",
      "64.1474417418\n",
      "7.96196964329\n",
      "[ 20.71405424  20.71405424  20.71405424 ...,  20.71405424  20.71405424\n",
      "  20.71405424]\n",
      "PHI\n",
      "0.2\n",
      "err sq and err perc\n",
      "264.844755635\n",
      "18.8763155216\n",
      "[ 22.60488845  22.60488845  22.60488845 ...,  22.60488845  22.60488845\n",
      "  22.60488845]\n",
      "PHI\n",
      "0.3\n",
      "err sq and err perc\n",
      "230.595495288\n",
      "14.9861028567\n",
      "[ 23.58514031  21.18662987  21.18662987 ...,  21.18662987  21.18662987\n",
      "  21.18662987]\n",
      "PHI\n",
      "0.4\n",
      "err sq and err perc\n",
      "232.502988882\n",
      "15.8585097573\n",
      "[ 23.24256256  23.24256256  23.24256256 ...,  23.24256256  23.24256256\n",
      "  23.24256256]\n",
      "PHI\n",
      "0.5\n",
      "err sq and err perc\n",
      "246.955338636\n",
      "16.0755902091\n",
      "[  2.79226745  22.08132997   2.58448412 ...,  25.77896203  22.08132997\n",
      "  22.08132997]\n",
      "PHI\n",
      "0.6\n",
      "err sq and err perc\n",
      "183.013475487\n",
      "14.9116079345\n",
      "[ 22.04602917  22.04602917  22.04602917 ...,  22.04602917  22.04602917\n",
      "  22.04602917]\n",
      "PHI\n",
      "0.7\n",
      "err sq and err perc\n",
      "204.543695256\n",
      "14.5858252223\n",
      "[ 20.27683142  20.27683142  27.40814728 ...,  20.27683142  20.27683142\n",
      "  27.40814728]\n",
      "PHI\n",
      "0.8\n",
      "err sq and err perc\n",
      "234.844944201\n",
      "15.6776155988\n",
      "[ 2.84869341  2.84869341  2.84869341 ...,  2.84869341  2.84869341\n",
      "  2.84869341]\n",
      "PHI\n",
      "0.9\n",
      "err sq and err perc\n",
      "64.4441653237\n",
      "7.54648412954\n"
     ]
    }
   ],
   "source": [
    "#Cross Validate Paramters\n",
    "for p in [.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9]:\n",
    "    boost, pred = AdaBoostRT(X_train, y_train,  model = regr, phi = .8, xtest=X_test, maxIter=100)\n",
    "    print(pred)\n",
    "    print('PHI')\n",
    "    print(p)\n",
    "    print('err sq and err perc')\n",
    "    print(np.mean(np.abs((np.array(y_test).ravel() - np.array(pred).ravel()) / np.array(y_test).ravel())) * 100)\n",
    "    print(np.sqrt(sum(np.square(np.array(pred).ravel() - np.array(y_test).ravel())) / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
